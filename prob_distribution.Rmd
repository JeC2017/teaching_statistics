---
title: "<centering>主要な確率分布</centering>"
subtitle: 専修大学経済学部
author: 陳 ショウジ
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  prettydoc::html_pretty:
    theme: cayman 
    highlight: github
    math: katex
    number_sections: no
    toc: yes
    toc_depth: 3
    self_contained: yes
  pdf_document: 
    
    toc: yes
keywords: 
abstract: 推測統計では、母集団から無作為抽出されるデ一タを確率変数ととらえ、母集団分布の性質を推定します。 デ一タによっては、あらかじめ母集団分布の形状が明らかな場合があります。 そうした場合、デ一タから母集団の特性値 (中心、ばらつきなど) を推定したり、その確率的評価をしたりすることが容易です。 本章では、離散確率分布としてベルヌ一イ分布と二項分布、連続確率分布として正規分布を紹介します。 また、母集団分布の形状が分からない場合でも、確率変数の和や平均の分布は正規分布で近似できること (中心極限定理) を説明します。 参考書籍 一 藪友良 (2012)，『入門 実践する統計学』、東洋経済新報社。
editor_options:
  chunk_output_type: console
---
```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, comment=NA)
# Clear workspace
rm(list = ls())
# Set seed for reproducibility
set.seed(1234)
```

```{r, echo=FALSE}
# ユーザーのワークスペースから R の既存のオブジェクトを削除する
rm(list=ls())

# Set directory (this should be set to user's own directory)
setwd("/Users/jauer/Dropbox/teaching/teaching_senshu/lectures/07_prob_distribution")
```

# 離散確率分布

## ベルヌ一イ分布

+ ある試行の結果、特定の事象 $A$ が起これば成功、それ以外であれば失敗とします。 たとえば、コインの表がでれば成功、裹なら失敗という具合です。
+ このとき $X$ は成功なら1、失敗なら0をとる確率変数とします。 また、それぞれの確率を $p$ と $1-p$ とします (歪みのないコインなら表の確率は $p=0.5$、裹の確率は $1-p=0.5$ です)。
+ このような $X$ をとくに**ベルヌ一イ確率変数**と呼び、その確率分布を**ベルヌ一イ分布**といいます。

> **ベルヌ一イ分布** (Bernoulli distribution)\par
$X$ は 0 か 1 の値をとり、$X=1$ の確率は $P(X=1)=p$、$X=0$ の確率は $P(X=0)=1-p$ となる。

+ ベルヌ一イ確率変数 $X$ の期待値は以下のとおりです。
$$
E[X] = 1\times p + 0\times (1-p) = p
$$
+ ベルヌ一イ確率変数 $X$ の分散は
$$
\begin{aligned}
V(X) &=E\left[\left(X -E[X] \right)^2 \right]\\
&= E\left[\left(X -p \right)^2 \right]\\
&= (1-p)^2\times p + (0-p)^2 \times (1-p)\\
&= p (1-p)
\end{aligned}
$$

## 二項分布

+ ベルヌ一イ分布と同様、ある試行の結果、特定の事象 $A$ が起これば成功、それ以外であれば失敗とし、成功の確率を $p$、失敗の確率を $1-p$ とします。 
+ この試行を $n$ 回繰り返すと、成功回数 $X$ は**二項確率変数**となり、その確率分布は**二項分布**となります。
+ たとえば、コインを $n$ 回投げ、表が出る回数を $X$ とすると、$X$ は二項確率変数となります。

> **二項分布** (Binominal distribution)\par
成功の確率を $p$、失敗の確率を $1-p$ とする試行を $n$ 回行うとき、$n$ 回中 $x$ 回成功する確率は
$$
\begin{aligned}
p(X=x) &= C^n_x p^x (1-p)^{n-x}\\
&= \frac{n!}{x! (n-x)!} p^x (1-p)^{n-x}
\end{aligned}
$$
となる。

+ 上式から明らかなように、二項分布は $n$ と $p$ によって完全に決定されます。
+ 換言すれば、$n$ と $p$ がその確率を決定する母数です。 このため、$X$ が二項分布に従うことを、$X\sim B(n, p)$ と表します ($B$ は Binominal の頭文字で二項分布を、「$\sim$」は従う意味します)。

+ 二項確率変数 $X$ は、相互に独立なベルヌ一イ確率変数の和となっています。
+ $X_1, X_2, \cdots, X_n$ を相互に独立なベルヌ一イ確率変数とすると $( P(X_i=1)=p, \ P(X_i=0)=1-p)$、$n$ 個のベルヌ一イ確率変数の和 $(X=X_1+X_2+\cdots + X_n = \sum_{i=1}^n X_i)$ は二項確率変数となります $( X\sim B(n,p))$。
+ たとえば、コインを3回投げて表が出る回数を $X$ とします。 このとき、$X_i$ を $i$ 回目にコインを投げて表なら 1、裹なら 0 となるベルヌ一イ確率変数とすると、$X= X_1+ X_2+X_3$ となります。
+ たとえば、1、2回目でけが表なら $X_1=1, \ X_2=1, \ X_3=0$ で、3回のうち表の回数は $X=X_1+X_2+X_3=1+1+0=2$ です。

> 二項確率変数 $X$ の期待値 $E[X]$ と分散 $V(X)$ は
$$
E[X] = np, \quad V(X)=np(1-p)
$$
で与えられます。

+ 前述のとおり、二項確率変数 $X$ は相互独立なベルヌ一イ確率変数 $X_i$ の和です $( E[X_i]=p, \ V(X)=p(1-p))$。 また、確率変数の和の期待値はそれぞれの期待値の和となり、和の分散はそれぞれの分散の和となります。
+ したがって、$X=X_1+\cdots + X_n$ の期待値は
$$
\begin{aligned}
E[X] = E[X_1 + \cdots +X_n] &= E[X_1] + \cdots +E[X_n]\\
&= p+ \cdots + p\\
&=np
\end{aligned}
$$
であり、分散は次のとおりです。
$$
\begin{aligned}
V(X) = V(X_1 + \cdots +X_n) &= V(X_1) + \cdots +V(X_n)\\
&=p(1-p)+\cdots+p(1-p)\\
&=np(1-p)
\end{aligned}
$$

**例. \ (サイコロを3回振る)** \ サイコロを振って1の目が出たら成功とします (成功確率は $p=1/6$)。 そして、サイコロを3回振って成功する回数を $X$ とします。 このとき、3回中 $x$ 回成功する確率は
$$
\begin{aligned}
P(X=x) &= C^3_x \left(p \right)^x \left(1-p \right)^{3-x}\\
&=  \frac{3!}{x! (3-x)!} \left(\frac{1}{6} \right)^x \left(\frac{5}{6} \right)^{3-x}
\end{aligned}
$$
となります。 たとえば、サイコロを3回振って1回だけ成功する確率は
$$
\begin{aligned}
P(X=1) &=  \frac{3!}{1! (3-1)!} \left(\frac{1}{6} \right)^1 \left(\frac{5}{6} \right)^{3-1}\\
&= 3 \left(\frac{1}{6} \right) \left(\frac{5}{6} \right)^{2}\\
&=0.347
\end{aligned}
$$
です。 この式が正しいことを確認するため、たとえば、最初だけ1の目が出たとしましょう (最初だけ成功)。 成功を $S$、失敗を $F$ と表すと $SFF$ という状況です。 この確率は $(1/6)\times (5/6)^2$ となります。 ただし、1回だけ成功するのは、$SFF$ という場合だけではなく、2回目、3回目に1の目が出る場合もあります $(FSF, \ FFS)$。 つまり、1回だけ成功する組合せは計3通りあり、3回中1回だけ成功する確率は $3\times (1/6)\times (5/6)^2$ となります。 同様に、$X=0, \ 2, \ 3$ の確率はそれぞれ以下のとおりです。
$$
\begin{aligned}
P(X=0) &=  \frac{3!}{0! (3-0)!} \left(\frac{1}{6} \right)^0 \left(\frac{5}{6} \right)^{3-0} = \left(\frac{5}{6} \right)^{3} = 0.579\\
P(X=2) &=  \frac{3!}{2! (3-2)!} \left(\frac{1}{6} \right)^2 \left(\frac{5}{6} \right)^{3-2} = 3 \left(\frac{1}{6} \right)^{2} \left(\frac{5}{6} \right) = 0.069\\
P(X=3) &=  \frac{3!}{3! (3-3)!} \left(\frac{1}{6} \right)^3 \left(\frac{5}{6} \right)^{3-3} = \left(\frac{1}{6} \right)^{3} = 0.005
\end{aligned}
$$

```{r}
# n! をRで計算します: prod(n:1)
# 3! をRで計算します
prod(3:1)

# 二項分布
# サイコロを3回振る: size = 3
x <- 0:3
plot(x, dbinom(x, size = 3, prob = 1/6), type="h")
```


# 連続確率分布

## 正規分布

釣鐘状の分布は**正規分布**と呼ばれます。 正規分布は指数関数の代表的な底(てい)であるネイピア数 $e=2.718\cdots$を使って、以下のように定義されます。

> **正規分布** (Normal distribution)\par
$X$ は $-\infty$ から $\infty$ までの値をとる連続確率変数であるため、その確率は密度関数の幅で与えられる。 その密度関数は
$$
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2 }
$$
であり、このときの $X$ を**正規確率変数**、その分布を**正規分布**という。

+ 正規確率変数 $X$ の期待値は $E[X]=\mu$、分散は $V(X)=\sigma^2$ となります。
+ 密度関数の式から明らかなように、正規分布の形状は $\mu$ と $\sigma^2$ によって完全に決定されます。
+ 換言すると、$\mu$ と $\sigma^2$ が密度関数を決定する母数となっています。 このため、$X$ が正規分布に従うことを、$X\sim N(\mu, \sigma^2)$ と表します ($N$ は Normal の頭文字で正規分布を意味します)。

ここで、正規分布が $\mu$ を中心とした釣鐘状となる理由を考ましょう。

+ 第1は、分布が $\mu$ を中心として左右対称となる理由についてです。 密度関数の $e^{-(x-\mu)^2/2\sigma^2}$ という項は、$x=\mu$ のとき $e^0=1$ で最大となり、$x\neq \mu$ のとき 1 より小さく、$x$ が $\mu$ から離れるにつれて小さくなっていきます。 したがって、密度関数も $x=\mu$ 最大となり、$x$ が $\mu$ から離れると小さくなっていきます。 また、$(x-\mu)^2$ は $x=\mu$ を中心に左右対称ですか
+ 第2は、分布のばらつきが $\sigma^2$ に依存している理由についてです。 密度関数は $x=\mu$ で最大値をとり、$1/\sqrt{2\pi\sigma^2}$  となります ($e^0=1$)。 したがって、$\sigma^2$ が大きいほど密度関数の頂点は低くなり、$\sigma^2$ が小さいほどその頂点は高くなります。 いずれの場合も確率の和 (つまり密度関数の面積) は 1 ですから、頂点が低い場合には分布は広がり、頂点が高い場合には分布は $\mu$ の周りに集中します。 つまり $\sigma^2$ が大きいほど分布のばらつきが大きく、$\sigma^2$ が小さいほど分布のばらつきは小さくなります。

```{r}
x <- seq(-7,7,0.1)
# dnorm(): 正規確率変数の密度関数; mean, sd: standard deviation
# ばらつき = sd^2= 1
plot(x, dnorm(x, mean=0, sd=1), type="l")
# ばらつき = sd^2= 4
lines(x, dnorm(x, mean=0, sd=2), col="blue")
```

## 標準正規分布

正規確率変数のうち、期待値が 0、分散が 1 のものをとくに**標準正規確率変数**といい、その分布を**標準正規分布**と呼びます (標準正規分布の密度関数は、正規分布の密度関数に $\mu=0$、$\sigma^2=1$ を代入したもも)。

> **標準正規分布** (Standard normal distribution)\par
$Z$ は期待値 0、分散 1 の正規分布に従う。 $Z$ の密度関数は以下となる。
$$
f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}
$$

+ $Z$ が標準正規分布に従うことを、$Z\sim N(0,1)$ と表します。
+ 標準正規確率変数 $Z$ がある値 $z$ より小さい確率 $P(Z<z)$ を $R$で計算できます。
+ また、連続確率変数 $Z$ が1点をとる確率は 0 ですから $P(Z<z) = P(Z\leq z)$ となり、等号 $=$ の有無は確率に全く影響を与えません。

```{r}
x <- seq(-4,4,0.1)
# dnorm(): 正規確率変数の密度関数; mean, sd: standard deviation
plot(x, dnorm(x, mean=0, sd=1), type="l")

# P(Z < 1.96) = 0.975
pnorm(1.96)
```

## 正規分布の標準化

一般的な正規確率変数 $X$ の分布 $N(\mu, \sigma^2)$ は $\mu$ や $\sigma^2$ に依存していますが、$X$ の**標準化** (期待値を引き、標準偏差で割ると) によって、標準正規分布を用いた確率の計算が可能となります。

> **正規確率変数の標準化**\par
$X\sim N(\mu, \sigma^2)$ のとき、 $X$ から期待値 $\mu$ を引き、標準偏差 $\sigma$ で割って標準化した変数 $Z =  (X-\mu)/\sigma$ は標準正規分布に従う。
$$
Z = \frac{X-\mu}{\sigma}\sim N(0,1)
$$

+ ここで $(X-\mu)/\sigma$ は標準正規分布に従うため、$Z$と表しています。
+ たとえば、$X\sim N(1, 5^2)$ なら $Z=(X-1)/5 \sim N(0,1)$。 また $X\sim N(7, 15^2)$ なら $Z=(X-7)/15 \sim N(0,1)$ となります。
+ まず、$X$ の標準化によって、$Z=(X-\mu)/\sigma$ の期待値が 0 で分散が 1 となることを確認しましょう。 $Z=(X-\mu)/\sigma$ の期待値は、$E[X]=\mu$ を使って
$$
E\left[\frac{X-\mu}{\sigma} \right] = \frac{1}{\sigma}E[X-\mu]=\frac{1}{\sigma}\left(E[X] -\mu \right)=0
$$
となります。 $Z=(X-\mu)/\sigma$ の分散は、$V(X) = E\left[(X-\mu)^2 \right]=\sigma^2$ を用いて
$$
E\left[\left(\frac{X-\mu}{\sigma} -0 \right)^2 \right]
=\frac{1}{\sigma^2}E\left[(X-\mu)^2 \right]=\frac{1}{\sigma^2}\sigma^2=1
$$
となります。

## 中心極限定理

相互に独立な $n$ 個の確率変数 $X_1, X_2, \cdots, X_n$ が与えられたとき、確率変数 $X_i$ の分布がどのような形状であっても、$n$ が十分に大きければ、これらの和や平均は正規分布に従います。 これを**中央極限定理**といいます。

> **中央極限定理** (Central limit theorem)\par
相互に独立な $n$ 個の確率変数 $X_1, X_2, \cdots, X_n$ がある ($E[X_i]=\mu$、$V(X_i)=\sigma^2$)。 $n$ が十分に大きければ、これらの和と平均は正規分布に従う。
$$
\begin{aligned}
\sum_{i=1}^n X_i &\sim N(n\mu, \ n\sigma^2)\\
\bar{X} &\sim N\left(\mu, \ \frac{\sigma^2}{n}\right)
\end{aligned}
$$

**中心極限定理のシミュレ一ション**

中心極限定理をシミュレ一ションで実感するRスクりプトを解説します。
設定: 二項確率変数は、相互に独立なベルヌ一イ確率変数の和となっています。
$X_1, X_2, \cdots, X_n$ を相互に独立なベルヌ一イ確率変数とすると $( P(X_i=1)=p, \ P(X_i=0)=1-p)$、$n$ 個のベルヌ一イ確率変数の和 $\sum_{i=1}^n X_i$ は二項確率変数となります $( \sum_{i=1}^n X_i \sim B(n,p))$。
すると、$E[ \sum_{i=1}^n X_i] = np, \ V( \sum_{i=1}^n X_i)=np(1-p)$。

```{r}
# 標本サイズを指定
n <- 5
# 抽出回数を指定します
M <- 10000
# mean.d で n 個のベルヌ一イ確率変数の平均値(二項確率変数/n)を格納
mean.d <- numeric(M)
# for ル一プで { } の過程を n 回繰返します
for (k in 1:M) {
  #ベルヌ一イ確率変数 (p=0.25) を抽出します
  bernoulli <- sample(c(1,0), size = n, replace = TRUE, prob = c(0.25, 0.75))
  # n 個確率変数の平均
  mean.d[k] <- mean(bernoulli)
}
# 頻度分布で描画: 中心極限定理 n=5
hist(mean.d)
```

```{r}
# 標本サイズを指定
n <- 20
# 抽出回数を指定します
M <- 10000
# mean.d で n 個のベルヌ一イ確率変数の平均値(二項確率変数/n)を格納
mean.d <- numeric(M)
# for ル一プで { } の過程を n 回繰返します
for (k in 1:M) {
  #ベルヌ一イ確率変数 (p=0.25) を抽出します
  bernoulli <- sample(c(1,0), size = n, replace = TRUE, prob = c(0.25, 0.75))
  # n 個確率変数の平均
  mean.d[k] <- mean(bernoulli)
}
# 頻度分布で描画: 中心極限定理 n=20
hist(mean.d)
```


```{r}
# 標本サイズを指定
n <- 500
# mean.d で n 個のベルヌ一イ確率変数の平均値(二項確率変数/n)を格納
M <- 10000
# 二項確率変数
mean.d <- numeric(M)
# for ル一プで { } の過程を n 回繰返します
for (k in 1:M) {
  #ベルヌ一イ確率変数 (p=0.25) を抽出します
  bernoulli <- sample(c(1,0), size = n, replace = TRUE, prob = c(0.25, 0.75))
  # n 個確率変数の平均
  mean.d[k] <- mean(bernoulli)
}
# 頻度分布で描画: 中心極限定理 n=500
hist(mean.d)
```


# 練習問題

 $n$ 個の相互に独立な確率変数 $X_1, X_2, \cdots, X_n$ があり、$X_i$ の期待値を $\mu$、分散を $\sigma^2$ とします。$\bar{X}=\sum_{i=1}^n X_i /n$。 

+ [1] $E\left[\sum_{i=1}^n X_i \right]=n\mu$ と $E[\bar{X}]=\mu$ を証明してください。 

+ [2] $V\left(\sum_{i=1}^n X_i \right)=n\sigma^2$ と $V(\bar{X})=\sigma^2/n$ を証明してください。 








<a href="#top">`トップへ戻る`</a>


